{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdWcP1gbH6CDI3ZFtVn+fd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Denysse-Sevilla/MAT-421/blob/main/Module_D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework #4- Denysse Sevilla**"
      ],
      "metadata": {
        "id": "eoVv2JLurVsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1.1: Introduction"
      ],
      "metadata": {
        "id": "UbgSvfPXrZvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear algebra is commonly used in machine learning and data science. As a result, a basic understanding is essential to understanding the algotithms used in these fields."
      ],
      "metadata": {
        "id": "rGrayk3OrmVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1.2: Elements of Linear Algebra"
      ],
      "metadata": {
        "id": "_GcL9x-wrsMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.1 Linear Spaces"
      ],
      "metadata": {
        "id": "P-Ghc89Nr1Ih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **linear subspace** is made from linear combinations.\n",
        "\n",
        "\\\n",
        "A linear subspace of V is a subset U ⊆ V and upholds the following properties for all $u⃗_1, u⃗_2 ∈ U $ and $α∈ℝ$:\n",
        "\n",
        "\n",
        "*   **Closed under Addition**: the sum of two vectors from the space remaind in the subspace\n",
        "  - $ u⃗_1 + u⃗_2 ∈ U $\n",
        "*   **Closed under scaler multiplication**: The product of a scalar and a vector from the subspace remains in the subspace\n",
        "  - $α u⃗_1 ∈ U $\n",
        "* **Contains the vector 0**: the zero vector is in the subspace\n",
        "\n",
        "\\\n",
        "The **span** of a set of vectors is the collection of all possible linear combinations. Meaning:\n",
        "\n",
        "span $ (u⃗_1, ... ,u⃗_n) = {∑_{j-1}^n \\alpha_j u⃗_j  : \\alpha_1, ... , \\alpha_n \\in ℝ}.$\n",
        "\n",
        "\\\n",
        "**Lemma**: Every span is a linear subspace.\n",
        "\n",
        "\\\n",
        "The **column space** of a matrix $A∈ℝ^{n×m}$ is the span of the columns of A. Meaning:\n",
        "\n",
        "col(A) $ =$ span $(a⃗_1, ... , a⃗_m ) \\in ℝ^n $\n",
        "\n",
        "\\\n",
        "A list of vectors **$u_1$**, ... ,**$u_m$** that cannot be rewritten as a linear combination of each other is **linearly independent**. Meaning,\n",
        "\n",
        "$\\ ∀_i, u⃗_1 ∉$ span $\\ (\\{  u⃗_1 : j\\neq i\\})$\n",
        "\n",
        "\\\n",
        "**Lemma**: The vectors are linearly independent iff\n",
        "\n",
        "$\\ ∑_{j=1}^{m} α_j u⃗_j =$ **$\\ 0$** $\\ ⟹ α_j = 0, ∀_j $\n",
        "\n",
        "\\\n",
        "If a list of vectors are not linearly independednt, then they are **linearly dependent**.\n",
        "\n",
        "\\\n",
        "**Lemma**: The vectors are linearly dependent if and only if\n",
        "\n",
        "$\\ ∑_{j=1}^{m} α_j u⃗ = $ **$0$** , where there exists $\\ α_j$'s, not all zero.\n",
        "\n",
        "\\\n",
        "The **basis** of a space is a unique representation of the vectors in the subspce. A basis of $U$ is a list of vectors $ u⃗_1, ... , u⃗_m$ in $U$ that fulfill the following prerequisites:\n",
        "\n",
        "* The vectors span($U$)\n",
        "* The vectors are linearly independent\n",
        "\n",
        "\\\n",
        "ALthough a vector space can have several bases, all bases must have the same dimension, or number of elements. In terms of matrices, the dimension of a column space is referred to as the **rank** of the matrix. The rank is essentially the number of linearly independent columns or rows in a matrix.\n",
        "\n",
        "**Note**: The number of linearly indepedent rows always equals the number of independent columns for any matrix!\n",
        "\n",
        "\\\n",
        "**Lemma**: Let $U$ be a linear subspace of $V$. Then, any basis of $U$ and all its bases have the same number of elements (length). This number is the dimension of $U$, denoted dim($U$).\n",
        "\n",
        "\\\n",
        "**Lemma**: One of the vectors from a  linearly dependent list of vectors is in the span of the previous ones. So, we can remove this vector without changing the span.\n"
      ],
      "metadata": {
        "id": "AvWzxT8bUifg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.2 Orthogonality"
      ],
      "metadata": {
        "id": "Cmsf4RoIUcUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A list of vectors $ {u⃗_1, ... , u⃗_m}$ is **orthonormal** if the dot product of any two distinct vectors is 0 (orthogonal) and is normalized (norm 1). Meaning:\n",
        "\n",
        "* $ ⟨u⃗_i, u⃗_j ⟩ = 0$\n",
        "* $ ||u⃗_i || = 1$\n",
        "\n",
        "\\\n",
        "**Lemma**: Let $ {u⃗_1, ..., u⃗_m}$ be an orthonormal list of vectors. Then,\n",
        "\n",
        "1. $ || ∑_{j=1}^m α_j u⃗_j ||^2 = ∑_{j=1}^m α_j^2$ for any $α_j \\in ℝ, j \\in {1, ..., m}$\n",
        "2. ${u⃗_1, ... , u⃗_m}$ are linearly independent\n",
        "\n",
        "\\\n",
        "**Lemma**: Let $k⃗_1, ... , k⃗_m$ be an orthonormal basis of $ 𝒰$ and $u⃗ \\in 𝒰$. Then,\n",
        "\n",
        "$u⃗ = ∑_{j=1}^m ⟨u⃗, k⃗_j ⟩k⃗_j$.\n",
        "\n",
        "\\\n",
        "Let $𝒰 ⊆ V$ be a linear subspace with orthonormal basis $k⃗_1, ... , k⃗_m$. Then, the **orthonormal projection** of $ v⃗ \\in V$ on $ 𝒰$ is defined as\n",
        "\n",
        "$ 𝒫_𝒰 v⃗ = ∑_{j=1}^m ⟨v⃗, k⃗_j ⟩k⃗_j$.\n",
        "\n",
        "\\\n",
        "**Best Approximation Theorem**: Let $𝒰 ⊆ V$ be a linear subspce with orthonormal basis $ k⃗_1, ... , k⃗_m$ and let $ v⃗ \\in V$ . Then for any $u⃗ \\in 𝒰$ :\n",
        "\n",
        "$ ||v⃗ - 𝒫_𝒰 v⃗ || ≤ || v⃗ - u⃗ ||$.\n",
        "\n",
        "Hence if $||v⃗ - 𝒫_𝒰 v⃗ || = || v⃗ - u⃗ ||$, then $u⃗ = 𝒫_𝒰 v⃗ $.\n",
        "\n",
        "\\\n",
        "**Lemma**: The Pythagorean Theorem states that if $u⃗, v⃗ ∈ V$ is orthogonal, then\n",
        "\n",
        "$|| u⃗ + v⃗||^2 = ||u⃗||^2 + ||v⃗||^2$.\n",
        "\n",
        "\\\n",
        "**Lemma**: The Cauchy-Schwarz inequality states that for any $u⃗, v⃗ ∈V$,\n",
        "\n",
        "$ |⟨u⃗,v⃗⟩| \\le ||u⃗||  ||v\\vec||$.\n",
        "\n",
        "\\\n",
        "**Lemma**: Let $𝒰 ⊆ V$ be a linear subspace with orthonormal basis $k⃗_1, ... , k⃗_m$ and let $v⃗ \\in V$. Then, for any $u⃗ \\in 𝒰$,\n",
        "\n",
        "$⟨v⃗ - 𝒫_𝒰 v⃗, u⃗⟩ = 0$.\n",
        "\n",
        "Meaning that $v⃗$ can be rewritten, or decomposed, as $ (v⃗ - 𝒫_𝒰 v⃗) + 𝒫_𝒰 v⃗$, where both terms are orthogonal.\n",
        "\n",
        "\\\n",
        "Since the map $𝒫_𝒰$ is linear,\n",
        "\n",
        "$𝒫_𝒰(αx⃗ + y⃗) = ∑_{j=1}^m ⟨αx⃗ + y⃗, k⃗_j⟩k⃗_j = ∑_{j=1}^m {α ⟨x⃗, k⃗_j⟩ + ⟨y⃗, k⃗_j⟩}k⃗_j = α𝒫_𝒰x⃗ + 𝒫_𝒰y⃗$.\n",
        "\n",
        "\\\n",
        "Hence, it can be written as an $n × m$ matrix $A$. Let\n",
        "\n",
        " $A$ = \\begin{pmatrix}\n",
        "        | &  & | \\\\\n",
        "        a⃗_1 &  & a⃗_m \\\\\n",
        "        | &  & |\n",
        "       \\end{pmatrix}\n",
        "\n",
        "Then,\n",
        "\n",
        "$A^T v⃗$ = \\begin{pmatrix}\n",
        "           ⟨v⃗, k⃗_1⟩ \\\\\n",
        "           ... \\\\\n",
        "           ⟨v⃗, k⃗_m⟩\n",
        "          \\end{pmatrix}\n",
        "\n",
        "This lists the coefficients in the expansion of$ 𝒫_𝒰v⃗$ over the basis $k⃗_1, ... , k⃗_m$. As a result,\n",
        "\n",
        "$ 𝒫 = AA^T$.\n",
        "\n",
        "Also, $ A^TA = I_{m \\times m} $ , where $I_{m \\times m} $ is the identity matrix.\n",
        "\n"
      ],
      "metadata": {
        "id": "nTT5dSW8OuLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.3 Gram-Schmidt Process"
      ],
      "metadata": {
        "id": "XMrn27ZUgafG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Gram-Schmidt** process is an algorithm that is used to create an orthonormal basis through the use of a linearly independent set of vectors. Essentially, the vectors are added one by one, but only after removing their orthogonal projection on the previous vectors. As a result, we obtain a span that has the same subspace. Additionally, the orthogonal decomposition makes sure that our outcome is orthogonal.\n",
        "\n",
        "**Lemma**: Let $ c⃗_1, ... , c⃗_m$ in $ℝ^n$ be linearly independent, then there exists an orthonormal basis $ k⃗_1, ... , k⃗_m$ of span$(c⃗_1, ... , c⃗_m)$."
      ],
      "metadata": {
        "id": "g-bhbAcUgkaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.4 Eigenvalues and Eigenvectors"
      ],
      "metadata": {
        "id": "q9xB-WPCjmDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $M \\in ℝ^{n × n} $ be a square matrix. Then if there exists a nonzero vector $x⃗ \\neq $**$0$** such that\n",
        "\n",
        "$ Mx⃗ = λ x⃗$ ,\n",
        "\n",
        "$ λ \\in ℝ$ is an eigenvalue and $ x⃗\n",
        "$ is the eigenvector.\n",
        "\n",
        "Note: Not every matrix has an eigenvalue!\n",
        "\n",
        "\\\n",
        "**Lemma**: Let $M \\in ℝ^{n × n} $ and $ λ_1, ... , λ_m$ be distinct eigenvalues of $M$ with corresponding nonzero eigenvectors $x⃗_1, ... , x⃗_m$. Then, the eigenvectors are linearly independent and $m \\le d$.\n",
        "\n",
        "\\\n",
        "**Diagonal matrices** are denoted as diag$(λ_1, ... , λ_m)$. As the name suggests, the eigenvalues $λ_1, ... , λ_m$ are located on the diagonal of the matrices.\n",
        "\n",
        "\\\n",
        "**Lemma**: If $M$ is symmetric, then any two eigenvectors from different eigenspaces are orthogonal.\n",
        "\n",
        "\\\n",
        "A matrix $A$ is **similar** to a diagonal matrix $D$ if there exists an invertible matrix $P$ such that\n",
        "\n",
        "$ A=PDP^{-1}$.\n",
        "\n",
        "If $p⃗_1, ... , p⃗_m$ are the columns of $P$, then\n",
        "\n",
        "$AP =PD$\n",
        "\n",
        "or\n",
        "\n",
        "$Ap⃗_i = λ_ip⃗_i$.\n",
        "\n",
        "\\\n",
        "A matrix $A$ is **orthogonally diogonalizable** if there exists an orthogonal matrix $P$ and a diagonal matrix $D$ such that\n",
        "\n",
        "$ A = PDP^T = PDP^{-1} $ , since $P^{-1} = P^T$\n",
        "\n",
        "If A is orthogonally diagonalizable, then\n",
        "\n",
        "$A^T = (PDP^T)^T = P^{TT}D^TP^T = PDP^T = A$ ,\n",
        "\n",
        "meaning A is symmetric. This indicate sthat every symmetric matrix is orthogonally diagonalizable.\n",
        "\n",
        "\\\n",
        "**Lemma**: An $n×n$ symmetric matrix has the following properties:\n",
        "* The matrix has $n$ real eigenvalues\n",
        "* If λ is an eigenvalue of A with multiplicity $k$, then the eigenspace for $λ$ is $k$-dimensional\n",
        "* The eigenspaces are mutually orthogonal (eigenvectors corresponsing to different eigenvalues are orthogonal)\n",
        "* The matrix is orthogonally diagonalizable\n",
        "\n",
        "\\\n",
        "A **spectral decomposition** of a matrix $Q$ is\n",
        "\n",
        "$Q = λ_1v⃗_1v⃗_1^T + λ_2v⃗_2v⃗_2^T + ... + λ_mv⃗_mv⃗_m^T $ ,\n",
        "\n",
        "where each $v⃗_1v⃗_1^T$ is an $m×m$ matrix of rank 1.\n",
        "\n",
        "Notice how $Q$ is broken into terms determined by its eigenvlaues. This is why it is called spectral decomposition.\n",
        "\n",
        "\\\n",
        "**Lemma**: Let $A$ be an $n×n$ symmetric matrix with an orthogonal diagonalization $A = PDP^{-1}$. Then, the columns of $P$ are orthonormal eigenvectors $v⃗_1, ... , v⃗_n$ of $A$. Also, assume that the diagonal of $D$ are arranged in accending order so that $λ_1 \\le λ_2, ... , \\le λ_n$. Then,\n",
        "\n",
        "$min_{x\\neq 0} \\frac{x⃗^TAx⃗}{x⃗^Tx⃗} = λ_1$\n",
        "\n",
        "occurs when $x⃗ = v⃗_1$.\n",
        "\n",
        "On the other hand,\n",
        "\n",
        "$max_{x\\neq 0} \\frac{x⃗^TAx⃗}{x⃗^Tx⃗} = λ_n$\n",
        "\n",
        "occurs when $x⃗ = v⃗_n$."
      ],
      "metadata": {
        "id": "GjNP05hqjqLu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nBlpS38i6Mqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1.3: Linear Regression"
      ],
      "metadata": {
        "id": "n4mxEoSZ8M5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to the linear dependence between the models and their unknown parameters, **linear regression** is best used for its simplicity."
      ],
      "metadata": {
        "id": "5jZUafk-8UPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Section 1.3.1 QR Decomposition"
      ],
      "metadata": {
        "id": "-6JebBR48rbm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**QR Decomposition** is very useful for solving linear systems and least squares fitting. In order to derive QR Decomposition, the Gram-Schmidt process is used to create an orthonormal basis span.\n",
        "\n",
        "\\\n",
        "Let\n",
        "\n",
        "$A$ = \\begin{pmatrix}\n",
        "        | &  & | \\\\\n",
        "        a⃗_1 &  & a⃗_m \\\\\n",
        "        | &  & |\n",
        "       \\end{pmatrix}\n",
        "\n",
        "and\n",
        "\n",
        "$B$ = \\begin{pmatrix}\n",
        "        | &  & | \\\\\n",
        "        b⃗_1 &  & b⃗_m \\\\\n",
        "        | &  & |\n",
        "       \\end{pmatrix} ,\n",
        "\n",
        "where $A,B$ are $n×m$ matrices. These matrices can now be rewritten in the form\n",
        "\n",
        "$A = BR$ , where the $ith$ column of the $m×m$ matrix R holds the coefficients of the linear combination of $ b⃗_j$'s that create $ a⃗_i$. This new form is known as the QR decomoposition.\n",
        "\n",
        "Note: $R$ is upper triangular.\n"
      ],
      "metadata": {
        "id": "WtZAFXIo8zZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Section 1.3.2 Least-Squares Problems"
      ],
      "metadata": {
        "id": "H1Fe5iNp_PdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $ A \\in ℝ^{n×m}$ be an $n×m$ matrix and $b⃗ \\in ℝ^n$ be a vector. Then we use the least squares problem\n",
        "\n",
        "$min_{x⃗ \\in ℝ^m} || Ax⃗-b⃗||$\n",
        "\n",
        "to solve the system\n",
        "\n",
        "$Ax⃗ = b⃗$.\n",
        "\n",
        "\\\n",
        "We follow the following steps:\n",
        "1. Rewrite $A,b⃗$ into the following form\n",
        "\n",
        "  $A$ = \\begin{pmatrix}\n",
        "        | &  & | \\\\\n",
        "        a⃗_1 &  & a⃗_m \\\\\n",
        "        | &  & |\n",
        "       \\end{pmatrix} = \\begin{pmatrix}\n",
        "  a_{1,1} & ... & a_{1,m} \\\\\n",
        "  \\vdots & \\ddots & \\vdots \\\\\n",
        "  a_{n,1} & ... & a_{n,m}\n",
        "  \\end{pmatrix}\n",
        "\n",
        "  and\n",
        "\n",
        "  $b⃗$ = \\begin{pmatrix}\n",
        "       b_1 \\\\\n",
        "       \\vdots \\\\\n",
        "       b_n\n",
        "       \\end{pmatrix}\n",
        "\n",
        "2. Find a linear combination of the columns of A that minimizes\n",
        "\n",
        "  $|| ∑_{j=1}^m x_j a⃗_j -b⃗||^2 = ∑_{i=1}^n(∑_{j=1}^m x_ja_{i,j}-b_i)^2 = ∑_{i=1}^n(ŷ_i - b_i)^2$,\n",
        "\n",
        "  where $ŷ_i = ∑_{j=1}^m x_ja_{i,j}$.\n",
        "\n",
        "3. Apply our characterization of the orthogonal projection on the column space of $A$.\n",
        "  Let $b̂ = 𝒫_{col(A)}b⃗$. Then, there is an $x̂$ such that\n",
        "\n",
        "  $Ax̂ = b̂$.\n",
        "  \n",
        "  Since $b̂$ is the closed point in col$(A)$ to $b⃗$, $x̂$ is a least-square solution of $Ax⃗ = b⃗$ if and only if the following lemma applies:\n",
        "\n",
        "  **Lemma**: The solution to the least-squares problem\n",
        "\n",
        "  $min_{x⃗ \\in ℝ^m} ||Ax⃗-b⃗||$\n",
        "\n",
        "  satisfies\n",
        "  \n",
        "  $A^TAx⃗=A^Tb⃗$  \n",
        "  \n",
        "  (known as the normal equations).\n",
        "\n",
        "\\\n",
        "This approach may result in some numerical issues. Hence, below are the steps to solving th same problem using QR decomposition:\n",
        "\n",
        "1. Construct an orthogonal basis of col$(A)$ through QR decomposition.\n",
        "\n",
        "2. Form the orthogonal projection matrix\n",
        "\n",
        "  $ 𝒫_{col(A)} = QQ^T$\n",
        "\n",
        "3. Apply the projection to $b⃗$ and observe that $x⃗^*$ satisfies the following\n",
        "\n",
        "  $Ax⃗^* = QQ^Tb⃗$\n",
        "\n",
        "4. Use the QR decomposition for $A$ to obtain\n",
        "\n",
        "  $QRx⃗^* = QQ^Tb⃗$\n",
        "\n",
        "5. Multiply both sides by $Q^T$ to obtain\n",
        "\n",
        "  $Rx⃗^* = Q^T b⃗$\n",
        "\n",
        "  Note: $Q^TQ = I_{m×m}$\n",
        "\n",
        "6. Solve the system for $x⃗^*$ using back substitution.\n",
        "\n",
        "\n",
        "**Lemma**: The solution to the least-squares problem (via QR decomposition)\n",
        "\n",
        "$min_{x⃗ \\in ℝ^m} ||Ax⃗-b⃗||$\n",
        "\n",
        "satisfies\n",
        "\n",
        "$Rx⃗^* = Q^T b⃗$.\n"
      ],
      "metadata": {
        "id": "hik2mGYV_WJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Section 1.3.3 Linear Regression"
      ],
      "metadata": {
        "id": "sIfseRQ6IjKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The minimization problem can be rewritten in matrix form:\n",
        "\n",
        "$y⃗$ = \\begin{pmatrix}\n",
        "      y_1 \\\\\n",
        "      y_2 \\\\\n",
        "      \\vdots \\\\\n",
        "      y_n\n",
        "      \\end{pmatrix} ,\n",
        "\n",
        " $A$ =\\begin{pmatrix}\n",
        "      1 & x⃗_1^T \\\\\n",
        "      1 & x⃗_2^T \\\\\n",
        "      \\vdots & \\vdots\\\\\n",
        "      1 & x⃗_n^T\n",
        "      \\end{pmatrix} , and\n",
        "\n",
        "$ β$ = \\begin{pmatrix}\n",
        "      \\beta_0 \\\\\n",
        "      \\beta_1 \\\\\n",
        "      \\vdots \\\\\n",
        "      \\beta_d\n",
        "      \\end{pmatrix}.\n",
        "\n",
        "\n",
        "This transforms the problem to\n",
        "\n",
        "$min_β ||y⃗ - Aβ||^2$.\n",
        "\n",
        "\\\n",
        "**Linear regression**, is used by finding the values of $\\beta$ that minimize the following\n",
        "\n",
        "$∑_{i=1}^n (y_i-ŷ_i)^2$,\n",
        "\n",
        "where\n",
        "\n",
        "$ŷ_i = β_0 + ∑_{j=1}^d β_jx_{ij}$.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tqNb6s3KIoM8"
      }
    }
  ]
}