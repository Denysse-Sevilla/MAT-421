{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdWcP1gbH6CDI3ZFtVn+fd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Denysse-Sevilla/MAT-421/blob/main/Module_D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework #4- Denysse Sevilla**"
      ],
      "metadata": {
        "id": "eoVv2JLurVsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1.1: Introduction"
      ],
      "metadata": {
        "id": "UbgSvfPXrZvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear algebra is commonly used in machine learning and data science. As a result, a basic understanding is essential to understanding the algotithms used in these fields."
      ],
      "metadata": {
        "id": "rGrayk3OrmVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1.2: Elements of Linear Algebra"
      ],
      "metadata": {
        "id": "_GcL9x-wrsMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.1 Linear Spaces"
      ],
      "metadata": {
        "id": "P-Ghc89Nr1Ih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **linear subspace** is made from linear combinations.\n",
        "\n",
        "\\\n",
        "A linear subspace of V is a subset U âŠ† V and upholds the following properties for all $uâƒ—_1, uâƒ—_2 âˆˆ U $ and $Î±âˆˆâ„$:\n",
        "\n",
        "\n",
        "*   **Closed under Addition**: the sum of two vectors from the space remaind in the subspace\n",
        "  - $ uâƒ—_1 + uâƒ—_2 âˆˆ U $\n",
        "*   **Closed under scaler multiplication**: The product of a scalar and a vector from the subspace remains in the subspace\n",
        "  - $Î± uâƒ—_1 âˆˆ U $\n",
        "* **Contains the vector 0**: the zero vector is in the subspace\n",
        "\n",
        "\\\n",
        "The **span** of a set of vectors is the collection of all possible linear combinations. Meaning:\n",
        "\n",
        "span $ (uâƒ—_1, ... ,uâƒ—_n) = {âˆ‘_{j-1}^n \\alpha_j uâƒ—_j  : \\alpha_1, ... , \\alpha_n \\in â„}.$\n",
        "\n",
        "\\\n",
        "**Lemma**: Every span is a linear subspace.\n",
        "\n",
        "\\\n",
        "The **column space** of a matrix $Aâˆˆâ„^{nÃ—m}$ is the span of the columns of A. Meaning:\n",
        "\n",
        "col(A) $ =$ span $(aâƒ—_1, ... , aâƒ—_m ) \\in â„^n $\n",
        "\n",
        "\\\n",
        "A list of vectors **$u_1$**, ... ,**$u_m$** that cannot be rewritten as a linear combination of each other is **linearly independent**. Meaning,\n",
        "\n",
        "$\\ âˆ€_i, uâƒ—_1 âˆ‰$ span $\\ (\\{  uâƒ—_1 : j\\neq i\\})$\n",
        "\n",
        "\\\n",
        "**Lemma**: The vectors are linearly independent iff\n",
        "\n",
        "$\\ âˆ‘_{j=1}^{m} Î±_j uâƒ—_j =$ **$\\ 0$** $\\ âŸ¹ Î±_j = 0, âˆ€_j $\n",
        "\n",
        "\\\n",
        "If a list of vectors are not linearly independednt, then they are **linearly dependent**.\n",
        "\n",
        "\\\n",
        "**Lemma**: The vectors are linearly dependent if and only if\n",
        "\n",
        "$\\ âˆ‘_{j=1}^{m} Î±_j uâƒ— = $ **$0$** , where there exists $\\ Î±_j$'s, not all zero.\n",
        "\n",
        "\\\n",
        "The **basis** of a space is a unique representation of the vectors in the subspce. A basis of $U$ is a list of vectors $ uâƒ—_1, ... , uâƒ—_m$ in $U$ that fulfill the following prerequisites:\n",
        "\n",
        "* The vectors span($U$)\n",
        "* The vectors are linearly independent\n",
        "\n",
        "\\\n",
        "ALthough a vector space can have several bases, all bases must have the same dimension, or number of elements. In terms of matrices, the dimension of a column space is referred to as the **rank** of the matrix. The rank is essentially the number of linearly independent columns or rows in a matrix.\n",
        "\n",
        "**Note**: The number of linearly indepedent rows always equals the number of independent columns for any matrix!\n",
        "\n",
        "\\\n",
        "**Lemma**: Let $U$ be a linear subspace of $V$. Then, any basis of $U$ and all its bases have the same number of elements (length). This number is the dimension of $U$, denoted dim($U$).\n",
        "\n",
        "\\\n",
        "**Lemma**: One of the vectors from a  linearly dependent list of vectors is in the span of the previous ones. So, we can remove this vector without changing the span.\n"
      ],
      "metadata": {
        "id": "AvWzxT8bUifg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.2 Orthogonality"
      ],
      "metadata": {
        "id": "Cmsf4RoIUcUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A list of vectors $ {uâƒ—_1, ... , uâƒ—_m}$ is **orthonormal** if the dot product of any two distinct vectors is 0 (orthogonal) and is normalized (norm 1). Meaning:\n",
        "\n",
        "* $ âŸ¨uâƒ—_i, uâƒ—_j âŸ© = 0$\n",
        "* $ ||uâƒ—_i || = 1$\n",
        "\n",
        "\\\n",
        "**Lemma**: Let $ {uâƒ—_1, ..., uâƒ—_m}$ be an orthonormal list of vectors. Then,\n",
        "\n",
        "1. $ || âˆ‘_{j=1}^m Î±_j uâƒ—_j ||^2 = âˆ‘_{j=1}^m Î±_j^2$ for any $Î±_j \\in â„, j \\in {1, ..., m}$\n",
        "2. ${uâƒ—_1, ... , uâƒ—_m}$ are linearly independent\n",
        "\n",
        "\\\n",
        "**Lemma**: Let $kâƒ—_1, ... , kâƒ—_m$ be an orthonormal basis of $ ğ’°$ and $uâƒ— \\in ğ’°$. Then,\n",
        "\n",
        "$uâƒ— = âˆ‘_{j=1}^m âŸ¨uâƒ—, kâƒ—_j âŸ©kâƒ—_j$.\n",
        "\n",
        "\\\n",
        "Let $ğ’° âŠ† V$ be a linear subspace with orthonormal basis $kâƒ—_1, ... , kâƒ—_m$. Then, the **orthonormal projection** of $ vâƒ— \\in V$ on $ ğ’°$ is defined as\n",
        "\n",
        "$ ğ’«_ğ’° vâƒ— = âˆ‘_{j=1}^m âŸ¨vâƒ—, kâƒ—_j âŸ©kâƒ—_j$.\n",
        "\n",
        "\\\n",
        "**Best Approximation Theorem**: Let $ğ’° âŠ† V$ be a linear subspce with orthonormal basis $ kâƒ—_1, ... , kâƒ—_m$ and let $ vâƒ— \\in V$ . Then for any $uâƒ— \\in ğ’°$ :\n",
        "\n",
        "$ ||vâƒ— - ğ’«_ğ’° vâƒ— || â‰¤ || vâƒ— - uâƒ— ||$.\n",
        "\n",
        "Hence if $||vâƒ— - ğ’«_ğ’° vâƒ— || = || vâƒ— - uâƒ— ||$, then $uâƒ— = ğ’«_ğ’° vâƒ— $.\n",
        "\n",
        "\\\n",
        "**Lemma**: The Pythagorean Theorem states that if $uâƒ—, vâƒ— âˆˆ V$ is orthogonal, then\n",
        "\n",
        "$|| uâƒ— + vâƒ—||^2 = ||uâƒ—||^2 + ||vâƒ—||^2$.\n",
        "\n",
        "\\\n",
        "**Lemma**: The Cauchy-Schwarz inequality states that for any $uâƒ—, vâƒ— âˆˆV$,\n",
        "\n",
        "$ |âŸ¨uâƒ—,vâƒ—âŸ©| \\le ||uâƒ—||  ||v\\vec||$.\n",
        "\n",
        "\\\n",
        "**Lemma**: Let $ğ’° âŠ† V$ be a linear subspace with orthonormal basis $kâƒ—_1, ... , kâƒ—_m$ and let $vâƒ— \\in V$. Then, for any $uâƒ— \\in ğ’°$,\n",
        "\n",
        "$âŸ¨vâƒ— - ğ’«_ğ’° vâƒ—, uâƒ—âŸ© = 0$.\n",
        "\n",
        "Meaning that $vâƒ—$ can be rewritten, or decomposed, as $ (vâƒ— - ğ’«_ğ’° vâƒ—) + ğ’«_ğ’° vâƒ—$, where both terms are orthogonal.\n",
        "\n",
        "\\\n",
        "Since the map $ğ’«_ğ’°$ is linear,\n",
        "\n",
        "$ğ’«_ğ’°(Î±xâƒ— + yâƒ—) = âˆ‘_{j=1}^m âŸ¨Î±xâƒ— + yâƒ—, kâƒ—_jâŸ©kâƒ—_j = âˆ‘_{j=1}^m {Î± âŸ¨xâƒ—, kâƒ—_jâŸ© + âŸ¨yâƒ—, kâƒ—_jâŸ©}kâƒ—_j = Î±ğ’«_ğ’°xâƒ— + ğ’«_ğ’°yâƒ—$.\n",
        "\n",
        "\\\n",
        "Hence, it can be written as an $n Ã— m$ matrix $A$. Let\n",
        "\n",
        " $A$ = \\begin{pmatrix}\n",
        "        | &  & | \\\\\n",
        "        aâƒ—_1 &  & aâƒ—_m \\\\\n",
        "        | &  & |\n",
        "       \\end{pmatrix}\n",
        "\n",
        "Then,\n",
        "\n",
        "$A^T vâƒ—$ = \\begin{pmatrix}\n",
        "           âŸ¨vâƒ—, kâƒ—_1âŸ© \\\\\n",
        "           ... \\\\\n",
        "           âŸ¨vâƒ—, kâƒ—_mâŸ©\n",
        "          \\end{pmatrix}\n",
        "\n",
        "This lists the coefficients in the expansion of$ ğ’«_ğ’°vâƒ—$ over the basis $kâƒ—_1, ... , kâƒ—_m$. As a result,\n",
        "\n",
        "$ ğ’« = AA^T$.\n",
        "\n",
        "Also, $ A^TA = I_{m \\times m} $ , where $I_{m \\times m} $ is the identity matrix.\n",
        "\n"
      ],
      "metadata": {
        "id": "nTT5dSW8OuLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.3 Gram-Schmidt Process"
      ],
      "metadata": {
        "id": "XMrn27ZUgafG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Gram-Schmidt** process is an algorithm that is used to create an orthonormal basis through the use of a linearly independent set of vectors. Essentially, the vectors are added one by one, but only after removing their orthogonal projection on the previous vectors. As a result, we obtain a span that has the same subspace. Additionally, the orthogonal decomposition makes sure that our outcome is orthogonal.\n",
        "\n",
        "**Lemma**: Let $ câƒ—_1, ... , câƒ—_m$ in $â„^n$ be linearly independent, then there exists an orthonormal basis $ kâƒ—_1, ... , kâƒ—_m$ of span$(câƒ—_1, ... , câƒ—_m)$."
      ],
      "metadata": {
        "id": "g-bhbAcUgkaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.4 Eigenvalues and Eigenvectors"
      ],
      "metadata": {
        "id": "q9xB-WPCjmDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $M \\in â„^{n Ã— n} $ be a square matrix. Then if there exists a nonzero vector $xâƒ— \\neq $**$0$** such that\n",
        "\n",
        "$ Mxâƒ— = Î» xâƒ—$ ,\n",
        "\n",
        "$ Î» \\in â„$ is an eigenvalue and $ xâƒ—\n",
        "$ is the eigenvector.\n",
        "\n",
        "Note: Not every matrix has an eigenvalue!\n",
        "\n",
        "\\\n",
        "**Lemma**: Let $M \\in â„^{n Ã— n} $ and $ Î»_1, ... , Î»_m$ be distinct eigenvalues of $M$ with corresponding nonzero eigenvectors $xâƒ—_1, ... , xâƒ—_m$. Then, the eigenvectors are linearly independent and $m \\le d$.\n",
        "\n",
        "\\\n",
        "**Diagonal matrices** are denoted as diag$(Î»_1, ... , Î»_m)$. As the name suggests, the eigenvalues $Î»_1, ... , Î»_m$ are located on the diagonal of the matrices.\n",
        "\n",
        "\\\n",
        "**Lemma**: If $M$ is symmetric, then any two eigenvectors from different eigenspaces are orthogonal.\n",
        "\n",
        "\\\n",
        "A matrix $A$ is **similar** to a diagonal matrix $D$ if there exists an invertible matrix $P$ such that\n",
        "\n",
        "$ A=PDP^{-1}$.\n",
        "\n",
        "If $pâƒ—_1, ... , pâƒ—_m$ are the columns of $P$, then\n",
        "\n",
        "$AP =PD$\n",
        "\n",
        "or\n",
        "\n",
        "$Apâƒ—_i = Î»_ipâƒ—_i$.\n",
        "\n",
        "\\\n",
        "A matrix $A$ is **orthogonally diogonalizable** if there exists an orthogonal matrix $P$ and a diagonal matrix $D$ such that\n",
        "\n",
        "$ A = PDP^T = PDP^{-1} $ , since $P^{-1} = P^T$\n",
        "\n",
        "If A is orthogonally diagonalizable, then\n",
        "\n",
        "$A^T = (PDP^T)^T = P^{TT}D^TP^T = PDP^T = A$ ,\n",
        "\n",
        "meaning A is symmetric. This indicate sthat every symmetric matrix is orthogonally diagonalizable.\n",
        "\n",
        "\\\n",
        "**Lemma**: An $nÃ—n$ symmetric matrix has the following properties:\n",
        "* The matrix has $n$ real eigenvalues\n",
        "* If Î» is an eigenvalue of A with multiplicity $k$, then the eigenspace for $Î»$ is $k$-dimensional\n",
        "* The eigenspaces are mutually orthogonal (eigenvectors corresponsing to different eigenvalues are orthogonal)\n",
        "* The matrix is orthogonally diagonalizable\n",
        "\n",
        "\\\n",
        "A **spectral decomposition** of a matrix $Q$ is\n",
        "\n",
        "$Q = Î»_1vâƒ—_1vâƒ—_1^T + Î»_2vâƒ—_2vâƒ—_2^T + ... + Î»_mvâƒ—_mvâƒ—_m^T $ ,\n",
        "\n",
        "where each $vâƒ—_1vâƒ—_1^T$ is an $mÃ—m$ matrix of rank 1.\n",
        "\n",
        "Notice how $Q$ is broken into terms determined by its eigenvlaues. This is why it is called spectral decomposition.\n",
        "\n",
        "\\\n",
        "**Lemma**: Let $A$ be an $nÃ—n$ symmetric matrix with an orthogonal diagonalization $A = PDP^{-1}$. Then, the columns of $P$ are orthonormal eigenvectors $vâƒ—_1, ... , vâƒ—_n$ of $A$. Also, assume that the diagonal of $D$ are arranged in accending order so that $Î»_1 \\le Î»_2, ... , \\le Î»_n$. Then,\n",
        "\n",
        "$min_{x\\neq 0} \\frac{xâƒ—^TAxâƒ—}{xâƒ—^Txâƒ—} = Î»_1$\n",
        "\n",
        "occurs when $xâƒ— = vâƒ—_1$.\n",
        "\n",
        "On the other hand,\n",
        "\n",
        "$max_{x\\neq 0} \\frac{xâƒ—^TAxâƒ—}{xâƒ—^Txâƒ—} = Î»_n$\n",
        "\n",
        "occurs when $xâƒ— = vâƒ—_n$."
      ],
      "metadata": {
        "id": "GjNP05hqjqLu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nBlpS38i6Mqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1.3: Linear Regression"
      ],
      "metadata": {
        "id": "n4mxEoSZ8M5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to the linear dependence between the models and their unknown parameters, **linear regression** is best used for its simplicity."
      ],
      "metadata": {
        "id": "5jZUafk-8UPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Section 1.3.1 QR Decomposition"
      ],
      "metadata": {
        "id": "-6JebBR48rbm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**QR Decomposition** is very useful for solving linear systems and least squares fitting. In order to derive QR Decomposition, the Gram-Schmidt process is used to create an orthonormal basis span.\n",
        "\n",
        "\\\n",
        "Let\n",
        "\n",
        "$A$ = \\begin{pmatrix}\n",
        "        | &  & | \\\\\n",
        "        aâƒ—_1 &  & aâƒ—_m \\\\\n",
        "        | &  & |\n",
        "       \\end{pmatrix}\n",
        "\n",
        "and\n",
        "\n",
        "$B$ = \\begin{pmatrix}\n",
        "        | &  & | \\\\\n",
        "        bâƒ—_1 &  & bâƒ—_m \\\\\n",
        "        | &  & |\n",
        "       \\end{pmatrix} ,\n",
        "\n",
        "where $A,B$ are $nÃ—m$ matrices. These matrices can now be rewritten in the form\n",
        "\n",
        "$A = BR$ , where the $ith$ column of the $mÃ—m$ matrix R holds the coefficients of the linear combination of $ bâƒ—_j$'s that create $ aâƒ—_i$. This new form is known as the QR decomoposition.\n",
        "\n",
        "Note: $R$ is upper triangular.\n"
      ],
      "metadata": {
        "id": "WtZAFXIo8zZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Section 1.3.2 Least-Squares Problems"
      ],
      "metadata": {
        "id": "H1Fe5iNp_PdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $ A \\in â„^{nÃ—m}$ be an $nÃ—m$ matrix and $bâƒ— \\in â„^n$ be a vector. Then we use the least squares problem\n",
        "\n",
        "$min_{xâƒ— \\in â„^m} || Axâƒ—-bâƒ—||$\n",
        "\n",
        "to solve the system\n",
        "\n",
        "$Axâƒ— = bâƒ—$.\n",
        "\n",
        "\\\n",
        "We follow the following steps:\n",
        "1. Rewrite $A,bâƒ—$ into the following form\n",
        "\n",
        "  $A$ = \\begin{pmatrix}\n",
        "        | &  & | \\\\\n",
        "        aâƒ—_1 &  & aâƒ—_m \\\\\n",
        "        | &  & |\n",
        "       \\end{pmatrix} = \\begin{pmatrix}\n",
        "  a_{1,1} & ... & a_{1,m} \\\\\n",
        "  \\vdots & \\ddots & \\vdots \\\\\n",
        "  a_{n,1} & ... & a_{n,m}\n",
        "  \\end{pmatrix}\n",
        "\n",
        "  and\n",
        "\n",
        "  $bâƒ—$ = \\begin{pmatrix}\n",
        "       b_1 \\\\\n",
        "       \\vdots \\\\\n",
        "       b_n\n",
        "       \\end{pmatrix}\n",
        "\n",
        "2. Find a linear combination of the columns of A that minimizes\n",
        "\n",
        "  $|| âˆ‘_{j=1}^m x_j aâƒ—_j -bâƒ—||^2 = âˆ‘_{i=1}^n(âˆ‘_{j=1}^m x_ja_{i,j}-b_i)^2 = âˆ‘_{i=1}^n(yÌ‚_i - b_i)^2$,\n",
        "\n",
        "  where $yÌ‚_i = âˆ‘_{j=1}^m x_ja_{i,j}$.\n",
        "\n",
        "3. Apply our characterization of the orthogonal projection on the column space of $A$.\n",
        "  Let $bÌ‚ = ğ’«_{col(A)}bâƒ—$. Then, there is an $xÌ‚$ such that\n",
        "\n",
        "  $AxÌ‚ = bÌ‚$.\n",
        "  \n",
        "  Since $bÌ‚$ is the closed point in col$(A)$ to $bâƒ—$, $xÌ‚$ is a least-square solution of $Axâƒ— = bâƒ—$ if and only if the following lemma applies:\n",
        "\n",
        "  **Lemma**: The solution to the least-squares problem\n",
        "\n",
        "  $min_{xâƒ— \\in â„^m} ||Axâƒ—-bâƒ—||$\n",
        "\n",
        "  satisfies\n",
        "  \n",
        "  $A^TAxâƒ—=A^Tbâƒ—$  \n",
        "  \n",
        "  (known as the normal equations).\n",
        "\n",
        "\\\n",
        "This approach may result in some numerical issues. Hence, below are the steps to solving th same problem using QR decomposition:\n",
        "\n",
        "1. Construct an orthogonal basis of col$(A)$ through QR decomposition.\n",
        "\n",
        "2. Form the orthogonal projection matrix\n",
        "\n",
        "  $ ğ’«_{col(A)} = QQ^T$\n",
        "\n",
        "3. Apply the projection to $bâƒ—$ and observe that $xâƒ—^*$ satisfies the following\n",
        "\n",
        "  $Axâƒ—^* = QQ^Tbâƒ—$\n",
        "\n",
        "4. Use the QR decomposition for $A$ to obtain\n",
        "\n",
        "  $QRxâƒ—^* = QQ^Tbâƒ—$\n",
        "\n",
        "5. Multiply both sides by $Q^T$ to obtain\n",
        "\n",
        "  $Rxâƒ—^* = Q^T bâƒ—$\n",
        "\n",
        "  Note: $Q^TQ = I_{mÃ—m}$\n",
        "\n",
        "6. Solve the system for $xâƒ—^*$ using back substitution.\n",
        "\n",
        "\n",
        "**Lemma**: The solution to the least-squares problem (via QR decomposition)\n",
        "\n",
        "$min_{xâƒ— \\in â„^m} ||Axâƒ—-bâƒ—||$\n",
        "\n",
        "satisfies\n",
        "\n",
        "$Rxâƒ—^* = Q^T bâƒ—$.\n"
      ],
      "metadata": {
        "id": "hik2mGYV_WJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Section 1.3.3 Linear Regression"
      ],
      "metadata": {
        "id": "sIfseRQ6IjKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The minimization problem can be rewritten in matrix form:\n",
        "\n",
        "$yâƒ—$ = \\begin{pmatrix}\n",
        "      y_1 \\\\\n",
        "      y_2 \\\\\n",
        "      \\vdots \\\\\n",
        "      y_n\n",
        "      \\end{pmatrix} ,\n",
        "\n",
        " $A$ =\\begin{pmatrix}\n",
        "      1 & xâƒ—_1^T \\\\\n",
        "      1 & xâƒ—_2^T \\\\\n",
        "      \\vdots & \\vdots\\\\\n",
        "      1 & xâƒ—_n^T\n",
        "      \\end{pmatrix} , and\n",
        "\n",
        "$ Î²$ = \\begin{pmatrix}\n",
        "      \\beta_0 \\\\\n",
        "      \\beta_1 \\\\\n",
        "      \\vdots \\\\\n",
        "      \\beta_d\n",
        "      \\end{pmatrix}.\n",
        "\n",
        "\n",
        "This transforms the problem to\n",
        "\n",
        "$min_Î² ||yâƒ— - AÎ²||^2$.\n",
        "\n",
        "\\\n",
        "**Linear regression**, is used by finding the values of $\\beta$ that minimize the following\n",
        "\n",
        "$âˆ‘_{i=1}^n (y_i-yÌ‚_i)^2$,\n",
        "\n",
        "where\n",
        "\n",
        "$yÌ‚_i = Î²_0 + âˆ‘_{j=1}^d Î²_jx_{ij}$.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tqNb6s3KIoM8"
      }
    }
  ]
}